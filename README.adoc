== Spring Cloud Data Flow Server - Local with Simple Storage Service (S3) App Registry Support

The Data Flow Server implementation that extends the https://github.com/spring-cloud/spring-cloud-dataflow/tree/master/spring-cloud-dataflow-server-local[Spring Cloud Data Flow Server - Local]
allowing the http://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html[Simple Storage Server (S3)] to be used as Application Registry in addition to the default Maven registry.
Allows registering apps http://docs.spring.io/spring-cloud-dataflow/docs/1.0.0.RELEASE/reference/html/getting-started-deploying-spring-cloud-dataflow.html#_deploying_local[stored in maven repositories] as well as S3 (Simple Store Storage)

This server is intended for development use only.

== Build
* Build from the spring-cloud-dataflow root directory:
+
----
./mvnw clean install
----

== How to Use
* Start Kafka locally (e.g. `kafka-server-start.sh /usr/local/etc/kafka/server.properties`)
* Start Redis locally via `redis-server`

=== Use static jars stored in S3
* Start the Local Data Flow Server application:
Provide the S3 credentials (e.g. --aws.accessKeyId=<Your-AWS-Access-Key> --aws.secretKey=<Your-AWS-Secret-Key>)
----
java -jar spring-cloud-dataflow-server-local-s3/target/spring-cloud-dataflow-server-local-s3-<version>.jar --aws.accessKeyId=<Your-AWS-Access-Key> --aws.secretKey=<Your-AWS-Secret-Key>
----
* Start the shell
----
$ java -jar spring-cloud-dataflow-shell/target/spring-cloud-dataflow-shell-<version>.jar
----
* Register app from S3
----
$ dataflow:>app register --name my-processor --type processor  --uri s3://my-bucket/snapshot/io/pivotal/my-processor/0.0.3-SNAPSHOT/my-processor-0.0.3-20160714.133004-1.jar
----

=== Use jars from S3 Maven Repository
* Start the Local Data Flow Server application:
Provide the S3 credentials and S3 maven repository location: `--s3.maven=s3://<bucket-name>/[snapshot|release]`
----
java -jar spring-cloud-dataflow-server-local-s3/target/spring-cloud-dataflow-server-local-s3-<version>.jar --aws.accessKeyId=<Your-AWS-Access-Key> --aws.secretKey=<Your-AWS-Secret-Key> --s3.maven=s3://my-s3-bucket/snapshot
----
* Start the shell
----
$ java -jar spring-cloud-dataflow-shell/target/spring-cloud-dataflow-shell-<version>.jar
----
* Register app S3 maven resource using the `s3-maven` protocol.
----
$ dataflow:>app register --name my-processor --type processor  --uri s3-maven://io.pivotal.walbrook:myprocessor:0.0.3-SNAPSHOT
----
Later will resolve the most recent my-repository artifact.

== Configuration
You need to provide S3 access credentials. In addition if yo use the s3 maven repository you have to provide it location too.

=== AWS S3 Access Credentials

Credentials would be needed to access the Amazon S3 app store. A credentials provider chain will be used that searches for credentials in this order:

* Spring Boot Command line Properties - **--aws.accessKeyId** and **--aws.secretKey**
* Environment Variables - **AWS_ACCESS_KEY_ID** and **AWS_SECRET_KEY**
* Java System Properties - **aws.accessKeyId** and **aws.secretKey**
* Credential profiles file at the default location (**~/.aws/credentials**) shared by all AWS SDKs and the AWS CLI
* Instance Profile Credentials - delivered through the Amazon EC2 metadata service

If no credentials are found in the chain, the server will attempt to work in an anonymous mode where requests aren't signed. For example if an Amazon S3 bucket has __Permission#Read__ permission for the __GroupGrantee#AllUsers__ group, the SCDF server can get app jar without credentials.

You can force the server to operate in an anonymous mode, and skip the credentials provider chain, by passing in **null** for the credentials.

=== AWS S3 Maven Repository

If you use the https://github.com/spring-projects/aws-maven[S3 Maven Repository] to deploy your build artifacts then you can
use the `s3-maven://` protocol tor resolve artifact from this repository using the groupId:artifactId:version schema.
For this you have to set s3 maven location like this: `--s3.maven=s3://<bucket-name>/[snapshot|release]`